{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters \n",
    "\n",
    "provider = 'CDS'\n",
    "var_X = 't2m'\n",
    "domain = 'ext_regional'\n",
    "target_var = 'TMEAN'\n",
    "target_type = 'cat_3'\n",
    "region_name = 'NNI'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load external modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "import pathlib\n",
    "from shutil import copytree, rmtree\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME = pathlib.Path.home()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import proplot as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load local modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../../../ml4seas/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import set_root_dir\n",
    "from GCM import get_GCM_outputs, shift_dset_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### domain definitions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_def = {}\n",
    "domain_def['local'] = [150, 200, -50, -10]\n",
    "domain_def['regional'] = [90, 300, -65, 50]\n",
    "domain_def['ext_regional'] = [70, 300, -70, 60]\n",
    "# domain_def['ext_regional'] = [50, 300, -75, 60]\n",
    "domain_def['global'] = [0, 360, -70, 70]\n",
    "domain_def['tropics'] = [0, 360, -40, 40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set the root path for the `data` folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpath = set_root_dir(root='gdata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "provider = 'CDS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCMs = ['ECMWF','UKMO','METEO_FRANCE']\n",
    "# GCMs = ['ECMWF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile ../../../ml4seas/GCM/concat_GCMs.py \n",
    "def concat_GCMs(provider, GCMs, var_name='T2M', period='hindcasts', rpath=None, domain='ext_regional', standardize=True, flatten=True, ensmean=True, step=3): \n",
    "    \"\"\"\n",
    "    Returns many GCM outputs concatenated along the time dimension\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    - provider : str, the provider in ['CDS','IRI','JMA'], no default \n",
    "    - GCMs : list, a list of GCMs in the provider \n",
    "    - period : the period to extract, in ['hindcasts','forecasts']\n",
    "    - rpath : str or pathlib.Path, the path to the 'data' folder \n",
    "    - domain : the domain, in ['local','regional','ext_regional', 'global', 'tropics']\n",
    "    - standardize : Boolean, must be True for 'hindcasts', False for 'forecasts'\n",
    "    - flatten : Boolean, whether or not to flatten the outputs along the spatial (and optionally members) dimension, default to True\n",
    "    - ensmean : Boolean, whether or not to calculate the ensemble mean, default to True\n",
    "    - step : the number of step by which to shift the time index, to align with observed target, default to 3 (assumes seasonal anomalies)\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    \n",
    "    - X_data_l : numpy.array containing the data concatenated along the time dimension (axis=0)\n",
    "    - X_data_l_std : if standardized=True, numpy.array containing the standardized data concatenated along the time dimension (axis=0)\n",
    "    - X_index_l :  numpy.array of Python datatimes, containing the index (note that repeated values will be present)\n",
    "    - GCM_records : numpy.array of len(X_index_l) containing the string for the corresponding GCM\n",
    "    - scalers_dict : if standardized=True, dictionnary, with each item (key = GCM) corresponding to fitted scikit-learn StandardScaler() object\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    import sys\n",
    "    import pathlib\n",
    "    import itertools\n",
    "    import numpy as np\n",
    "    \n",
    "    HOME = pathlib.Path.home()\n",
    "    \n",
    "    sys.path.append(HOME / 'research' / 'Smart_Ideas' / 'code' / 'ml4seas')\n",
    "    \n",
    "    from utils import set_root_dir\n",
    "    from GCM import get_GCM_outputs, shift_dset_time\n",
    "    \n",
    "    GCM_records = []\n",
    "    X_index_l = []\n",
    "    X_data_l = []\n",
    "    X_data_l_std = []\n",
    "\n",
    "    domain_def = {}\n",
    "    domain_def['local'] = [150, 200, -50, -10]\n",
    "    domain_def['regional'] = [90, 300, -65, 50]\n",
    "    domain_def['ext_regional'] = [70, 300, -70, 60]\n",
    "    # domain_def['ext_regional'] = [50, 300, -75, 60]\n",
    "    domain_def['global'] = [0, 360, -70, 70]\n",
    "    domain_def['tropics'] = [0, 360, -40, 40]    \n",
    "\n",
    "    if standardize: \n",
    "        \n",
    "        scalers_dict = {}\n",
    "    \n",
    "    if isinstance(rpath, str): \n",
    "        rpath = pathlib.Path(rpath)\n",
    "    \n",
    "    for GCM in GCMs: \n",
    "    \n",
    "        print(f\"getting {GCM}\")\n",
    "    \n",
    "        dset, coords = get_GCM_outputs(provider=provider, GCM=GCM, var_name=var_name, period=period, rpath=rpath, domain=domain_def[domain], step=step, flatten=flatten, ensmean=ensmean)\n",
    "        \n",
    "        if 'valid_time' in dset.coords: \n",
    "            dset = dset.drop('valid_time')        \n",
    "            \n",
    "        dset = shift_dset_time(dset, step=step)\n",
    "        \n",
    "        X_data = dset['t2m'].data\n",
    "        \n",
    "        X_index = dset['time'].to_index().to_pydatetime()\n",
    "        \n",
    "        if standardize: \n",
    "        \n",
    "            scaler = StandardScaler() \n",
    "\n",
    "            scaler = scaler.fit(X_data)\n",
    "\n",
    "            scalers_dict[GCM] = scaler\n",
    "\n",
    "            X_data_std = scaler.transform(X_data)\n",
    "\n",
    "        # append and records \n",
    "        \n",
    "        GCM_records.append(np.repeat([GCM], len(X_index)))\n",
    "        \n",
    "        X_index_l.append(X_index)\n",
    "        \n",
    "        X_data_l.append(X_data)\n",
    "\n",
    "        if standardize: \n",
    "        \n",
    "            X_data_l_std.append(X_data_std)\n",
    "        \n",
    "    GCM_records = np.array(list(itertools.chain(*GCM_records)))\n",
    "\n",
    "    X_index_l = np.array(list(itertools.chain(*X_index_l)))\n",
    "\n",
    "    X_data_l = np.array(list(itertools.chain(*X_data_l)))\n",
    "\n",
    "    if standardize: \n",
    "    \n",
    "        X_data_l_std = np.array(list(itertools.chain(*X_data_l_std)))\n",
    "\n",
    "        return X_data_l, X_data_l_std, X_index_l, GCM_records, scalers_dict\n",
    "    \n",
    "    else: \n",
    "        \n",
    "        return X_data_l, X_index_l, GCM_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpath = set_root_dir(root='gdata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting ECMWF\n",
      "/media/nicolasf/GDATA/END19101/Working/data/GCMs/processed/hindcasts/CDS/ECMWF/T2M\n",
      "288\n",
      "first file is /media/nicolasf/GDATA/END19101/Working/data/GCMs/processed/hindcasts/CDS/ECMWF/T2M/ECMWF_T2M_seasonal_anomalies_interp_1993_01.nc\n",
      "last file is /media/nicolasf/GDATA/END19101/Working/data/GCMs/processed/hindcasts/CDS/ECMWF/T2M/ECMWF_T2M_seasonal_anomalies_interp_2016_12.nc\n",
      "getting UKMO\n",
      "/media/nicolasf/GDATA/END19101/Working/data/GCMs/processed/hindcasts/CDS/UKMO/T2M\n",
      "287\n",
      "first file is /media/nicolasf/GDATA/END19101/Working/data/GCMs/processed/hindcasts/CDS/UKMO/T2M/UKMO_T2M_seasonal_anomalies_interp_1993_02.nc\n",
      "last file is /media/nicolasf/GDATA/END19101/Working/data/GCMs/processed/hindcasts/CDS/UKMO/T2M/UKMO_T2M_seasonal_anomalies_interp_2016_12.nc\n",
      "getting METEO_FRANCE\n",
      "/media/nicolasf/GDATA/END19101/Working/data/GCMs/processed/hindcasts/CDS/METEO_FRANCE/T2M\n",
      "288\n",
      "first file is /media/nicolasf/GDATA/END19101/Working/data/GCMs/processed/hindcasts/CDS/METEO_FRANCE/T2M/METEO_FRANCE_T2M_seasonal_anomalies_interp_1993_01.nc\n",
      "last file is /media/nicolasf/GDATA/END19101/Working/data/GCMs/processed/hindcasts/CDS/METEO_FRANCE/T2M/METEO_FRANCE_T2M_seasonal_anomalies_interp_2016_12.nc\n"
     ]
    }
   ],
   "source": [
    "X_data_train, X_data_train_std, X_index_train, GCM_records_train, scalers_dict = concat_GCMs(provider, GCMs, var_name='T2M', period='hindcasts', rpath=rpath, domain='ext_regional', standardize=True, flatten=True, ensmean=True, step=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpath = set_root_dir(root='local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting ECMWF\n",
      "/home/nicolasf/research/Smart_Ideas/data/GCMs/processed/forecasts/CDS/ECMWF/T2M\n",
      "36\n",
      "first file is /home/nicolasf/research/Smart_Ideas/data/GCMs/processed/forecasts/CDS/ECMWF/T2M/ECMWF_T2M_seasonal_anomalies_interp_2017_01.nc\n",
      "last file is /home/nicolasf/research/Smart_Ideas/data/GCMs/processed/forecasts/CDS/ECMWF/T2M/ECMWF_T2M_seasonal_anomalies_interp_2019_12.nc\n",
      "getting UKMO\n",
      "/home/nicolasf/research/Smart_Ideas/data/GCMs/processed/forecasts/CDS/UKMO/T2M\n",
      "28\n",
      "first file is /home/nicolasf/research/Smart_Ideas/data/GCMs/processed/forecasts/CDS/UKMO/T2M/UKMO_T2M_seasonal_anomalies_interp_2017_09.nc\n",
      "last file is /home/nicolasf/research/Smart_Ideas/data/GCMs/processed/forecasts/CDS/UKMO/T2M/UKMO_T2M_seasonal_anomalies_interp_2019_12.nc\n",
      "getting METEO_FRANCE\n",
      "/home/nicolasf/research/Smart_Ideas/data/GCMs/processed/forecasts/CDS/METEO_FRANCE/T2M\n",
      "36\n",
      "first file is /home/nicolasf/research/Smart_Ideas/data/GCMs/processed/forecasts/CDS/METEO_FRANCE/T2M/METEO_FRANCE_T2M_seasonal_anomalies_interp_2017_01.nc\n",
      "last file is /home/nicolasf/research/Smart_Ideas/data/GCMs/processed/forecasts/CDS/METEO_FRANCE/T2M/METEO_FRANCE_T2M_seasonal_anomalies_interp_2019_12.nc\n"
     ]
    }
   ],
   "source": [
    "X_data_test, X_index_test, GCM_records_test = concat_GCMs(provider, GCMs, var_name='T2M', period='forecasts', rpath=rpath, domain='ext_regional', standardize=False, flatten=True, ensmean=True, step=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 4929)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(863, 4929)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### apply the standard scalers to the test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ECMWF': StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       " 'UKMO': StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       " 'METEO_FRANCE': StandardScaler(copy=True, with_mean=True, with_std=True)}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalers_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_test_std = []\n",
    "for GCM in np.unique(GCM_records_test): \n",
    "    X_sub = X_data_test[GCM_records_test == GCM,:]\n",
    "    X_sub_std = scalers_dict[GCM].transform(X_sub)\n",
    "    X_data_test_std.append(X_sub_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_test_std = np.array(list(itertools.chain(*X_data_test_std)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 4929)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_test_std.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TARGETS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpath_target = HOME / 'research' / 'Smart_Ideas' / 'outputs' / 'targets' / 'NZ_regions' / 'NZ_6_regions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = []\n",
    "for reg in ['NNI','WNI','ENI','NSI','WSI','ESI']: \n",
    "    target = pd.read_csv(dpath_target / target_var / reg / f'TS_NZ_region_{reg}_{target_var}_3_quantiles_anoms.csv', index_col=0, parse_dates=True)\n",
    "    target.columns = pd.MultiIndex.from_product([[reg],target.columns])\n",
    "    targets.append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = pd.concat(targets, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">NNI</th>\n",
       "      <th colspan=\"3\" halign=\"left\">WNI</th>\n",
       "      <th colspan=\"3\" halign=\"left\">ENI</th>\n",
       "      <th colspan=\"3\" halign=\"left\">NSI</th>\n",
       "      <th colspan=\"3\" halign=\"left\">WSI</th>\n",
       "      <th colspan=\"3\" halign=\"left\">ESI</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Tmean_N</th>\n",
       "      <th>cat_3</th>\n",
       "      <th>anomalies</th>\n",
       "      <th>Tmean_N</th>\n",
       "      <th>cat_3</th>\n",
       "      <th>anomalies</th>\n",
       "      <th>Tmean_N</th>\n",
       "      <th>cat_3</th>\n",
       "      <th>anomalies</th>\n",
       "      <th>Tmean_N</th>\n",
       "      <th>cat_3</th>\n",
       "      <th>anomalies</th>\n",
       "      <th>Tmean_N</th>\n",
       "      <th>cat_3</th>\n",
       "      <th>anomalies</th>\n",
       "      <th>Tmean_N</th>\n",
       "      <th>cat_3</th>\n",
       "      <th>anomalies</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1979-03-31</th>\n",
       "      <td>18.278555</td>\n",
       "      <td>3</td>\n",
       "      <td>0.462528</td>\n",
       "      <td>16.051472</td>\n",
       "      <td>3</td>\n",
       "      <td>0.317965</td>\n",
       "      <td>16.732249</td>\n",
       "      <td>3</td>\n",
       "      <td>0.621540</td>\n",
       "      <td>13.811438</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.093327</td>\n",
       "      <td>11.848419</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.400334</td>\n",
       "      <td>13.728706</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.222255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979-04-30</th>\n",
       "      <td>16.794408</td>\n",
       "      <td>2</td>\n",
       "      <td>0.227319</td>\n",
       "      <td>14.586906</td>\n",
       "      <td>3</td>\n",
       "      <td>0.248038</td>\n",
       "      <td>14.953599</td>\n",
       "      <td>3</td>\n",
       "      <td>0.299057</td>\n",
       "      <td>12.189450</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.249176</td>\n",
       "      <td>10.589580</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.110858</td>\n",
       "      <td>12.033578</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.208919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979-05-31</th>\n",
       "      <td>14.695903</td>\n",
       "      <td>2</td>\n",
       "      <td>0.282907</td>\n",
       "      <td>12.522320</td>\n",
       "      <td>3</td>\n",
       "      <td>0.425773</td>\n",
       "      <td>12.716266</td>\n",
       "      <td>2</td>\n",
       "      <td>0.314655</td>\n",
       "      <td>9.888897</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.215657</td>\n",
       "      <td>8.099501</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.202497</td>\n",
       "      <td>9.232035</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.470303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979-06-30</th>\n",
       "      <td>12.093823</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.001099</td>\n",
       "      <td>9.888909</td>\n",
       "      <td>2</td>\n",
       "      <td>0.117671</td>\n",
       "      <td>9.929897</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.065854</td>\n",
       "      <td>7.198980</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.300772</td>\n",
       "      <td>5.457298</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.197458</td>\n",
       "      <td>6.634168</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.254247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979-07-31</th>\n",
       "      <td>10.290536</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.061355</td>\n",
       "      <td>8.182231</td>\n",
       "      <td>2</td>\n",
       "      <td>0.120974</td>\n",
       "      <td>8.208954</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.063564</td>\n",
       "      <td>5.534868</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.058724</td>\n",
       "      <td>3.763353</td>\n",
       "      <td>2</td>\n",
       "      <td>0.085515</td>\n",
       "      <td>4.916423</td>\n",
       "      <td>2</td>\n",
       "      <td>0.112719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  NNI                        WNI                        ENI  \\\n",
       "              Tmean_N cat_3 anomalies    Tmean_N cat_3 anomalies    Tmean_N   \n",
       "time                                                                          \n",
       "1979-03-31  18.278555     3  0.462528  16.051472     3  0.317965  16.732249   \n",
       "1979-04-30  16.794408     2  0.227319  14.586906     3  0.248038  14.953599   \n",
       "1979-05-31  14.695903     2  0.282907  12.522320     3  0.425773  12.716266   \n",
       "1979-06-30  12.093823     2 -0.001099   9.888909     2  0.117671   9.929897   \n",
       "1979-07-31  10.290536     2 -0.061355   8.182231     2  0.120974   8.208954   \n",
       "\n",
       "                                  NSI                        WSI        \\\n",
       "           cat_3 anomalies    Tmean_N cat_3 anomalies    Tmean_N cat_3   \n",
       "time                                                                     \n",
       "1979-03-31     3  0.621540  13.811438     2 -0.093327  11.848419     1   \n",
       "1979-04-30     3  0.299057  12.189450     1 -0.249176  10.589580     2   \n",
       "1979-05-31     2  0.314655   9.888897     1 -0.215657   8.099501     1   \n",
       "1979-06-30     1 -0.065854   7.198980     1 -0.300772   5.457298     1   \n",
       "1979-07-31     2 -0.063564   5.534868     2 -0.058724   3.763353     2   \n",
       "\n",
       "                            ESI                  \n",
       "           anomalies    Tmean_N cat_3 anomalies  \n",
       "time                                             \n",
       "1979-03-31 -0.400334  13.728706     2 -0.222255  \n",
       "1979-04-30 -0.110858  12.033578     2 -0.208919  \n",
       "1979-05-31 -0.202497   9.232035     1 -0.470303  \n",
       "1979-06-30 -0.197458   6.634168     1 -0.254247  \n",
       "1979-07-31  0.085515   4.916423     2  0.112719  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_anomalies = targets.loc[:, (slice(None), [\"anomalies\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_terciles = targets.loc[:, (slice(None), [\"cat_3\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_anomalies.columns = targets_anomalies.columns.droplevel(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_terciles.columns = target_terciles.columns.droplevel(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### target choice here (anomalies or tercile class, and region_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "if target_type == 'cat_3': \n",
    "    y = target_terciles.loc[:,region_name]\n",
    "elif target_type == 'anomalies': \n",
    "    y = target_anomalies.loc[:,region_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Now going agead with TARGET NNI, cat_3 ------------------------ \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n\\n\\nNow going agead with TARGET {region_name}, {target_type} ------------------------ \\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use the list of repeated index to select the target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y.loc[X_index_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(863,)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_index_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y.reindex(X_index_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now randomize "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get the shuffled indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_train_indexes = np.random.randint(0, len(X_index_train), len(X_index_train) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_test_indexes = np.random.randint(0, len(X_index_test), len(X_index_test) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### apply the shuffled indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_train_std_shuffled = X_data_train_std[shuffled_train_indexes,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_test_std_shuffled = X_data_test_std[shuffled_test_indexes,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_shuffled = y_train[shuffled_train_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_shuffled = y_test[shuffled_test_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TARGETS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpath_target = HOME / 'research' / 'Smart_Ideas' / 'outputs' / 'targets' / 'NZ_regions' / 'NZ_6_regions'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAINFALL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_var = 'TMEAN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = []\n",
    "for region_name in ['NNI','WNI','ENI','NSI','WSI','ESI']: \n",
    "    target = pd.read_csv(dpath_target / target_var / region_name / f'TS_NZ_region_{region_name}_{target_var}_3_quantiles_anoms.csv', index_col=0, parse_dates=True)\n",
    "    target.columns = pd.MultiIndex.from_product([[region_name],target.columns])\n",
    "    targets.append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = pd.concat(targets, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_anomalies = targets.loc[:, (slice(None), [\"anomalies\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_terciles = targets.loc[:, (slice(None), [\"cat_3\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_anomalies.columns = targets_anomalies.columns.droplevel(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_anomalies.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_terciles.columns = target_terciles.columns.droplevel(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_terciles.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "targets_anomalies.NNI.plot(ax=ax, lw=2)\n",
    "ax.grid(ls=':', color='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduce the dimensionality of the hindcasts / forecasts using PCA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the data in a numpy array "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loads in case the underlying data structures are dask arrays "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_t2m_ecmwf_hindcasts.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t2m_train = dset_t2m_ecmwf_hindcasts['t2m'].data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialise the standard scaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_t2m = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit and transform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t2m_train = scaler_t2m.fit_transform(X_t2m_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### verify that mean ~= 0 and std ~= 1 for all features (grid points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t2m_train.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t2m_train.std(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialise the PCA, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### percentage of variance we want to keep, scikit - learn will automatically select the number of PCs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_variance = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### initialisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skpca_t2m = pca.PCA(n_components=percent_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fit AND transform, returns the PCs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skpca_t2m_PCs_train = skpca_t2m.fit_transform(X_t2m_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### shape, number of pcs, do not forget that the 'member' dimension is also included in the z stacked dimension (member, lat, lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skpca_t2m_PCs_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pcs = skpca_t2m_PCs_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n_pcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gets the EOFs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eofs_t2m_train = skpca_t2m.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eofs_t2m_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_t2m_ecmwf_hindcasts.coords['z']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now calculate (project) the corresponding PCs in the forecast period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prior to that, transform using the Standard Scaler fitted over the hindcast period  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t2m_test =  dset_t2m_ecmwf_forecasts['t2m'].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t2m_test = scaler_t2m.transform(X_t2m_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### checks that the mean and the std are not too far off 0 and 1 respectively, note that due to temperarture trends, we expect an increase in the mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t2m_test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t2m_test.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### now transforms using the pca object fitted previously on the training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skpca_t2m_PCs_test = skpca_t2m.transform(X_t2m_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plots the PCs, casts these into a dataframe, with the correct time index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_skpca_t2m_PCs_train = pd.DataFrame(skpca_t2m_PCs_train, index=dset_t2m_ecmwf_hindcasts['time'].to_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_skpca_t2m_PCs_test = pd.DataFrame(skpca_t2m_PCs_test, index=dset_t2m_ecmwf_forecasts['time'].to_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_skpca_t2m_PCs_train.loc[:,0:10].plot(legend=None); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_skpca_t2m_PCs_test.loc[:,0:10].plot(legend=None); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eofs_t2m_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_hindcasts.dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eofs_t2m_train = eofs_t2m_train.reshape((n_pcs, coords_hindcasts.dims['member'], coords_hindcasts.dims['lat'], coords_hindcasts.dims['lon']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eofs_t2m_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### put this into a dataset with the right dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "d['pc'] = (('pc'), np.arange(n_pcs))\n",
    "d['member'] = coords_hindcasts['member']\n",
    "d['lat'] = coords_hindcasts['lat']\n",
    "d['lon'] = coords_hindcasts['lon'] \n",
    "d['eof'] = (('pc','member','lat','lon'), eofs_t2m_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eofs_train_dset = xr.Dataset(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plots all the EOFs along the member dimension "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eofs_train_dset.sel(pc=0)['eof'].plot(x='lon',y='lat', col='member', col_wrap=5, add_colorbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal component associated with the first EOF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_skpca_t2m_PCs_train.loc[:,0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_terciles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier as KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_score = {}\n",
    "f1_score  = {}\n",
    "\n",
    "\n",
    "for region_name in ['NNI','WNI','ENI','NSI','WSI','ESI']: \n",
    "    \n",
    "    train = df_skpca_t2m_PCs_train.copy()\n",
    "    train = train.merge(target_terciles.loc[:,region_name], left_index=True, right_index=True)\n",
    "    \n",
    "    test = df_skpca_t2m_PCs_test.copy()\n",
    "    test = test.merge(target_terciles.loc[:,region_name], left_index=True, right_index=True)\n",
    "    \n",
    "    X_train = train.iloc[:,:-1].values\n",
    "    y_train = train.iloc[:,-1].values\n",
    "    \n",
    "    X_test = test.iloc[:,:-1].values\n",
    "    y_test = test.iloc[:,-1].values\n",
    "    \n",
    "    knn = KNN(n_neighbors=5, metric='minkowski', weights='distance', p=1)\n",
    "    \n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    acc_score[region_name] = knn.score(X_test, y_test)\n",
    "    \n",
    "    y_test_pred_prob = knn.predict_proba(X_test)\n",
    "    \n",
    "    y_test_pred = knn.predict(X_test)\n",
    "    \n",
    "    f1_score[region_name] = metrics.f1_score(y_test, y_test_pred, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_score_dict = {}\n",
    "\n",
    "for region_name in ['NNI','WNI','ENI','NSI','WSI','ESI']: \n",
    "    \n",
    "    train = df_skpca_t2m_PCs_train.copy()\n",
    "    train = train.merge(target_terciles.loc[:,region_name], left_index=True, right_index=True)\n",
    "\n",
    "    test = df_skpca_t2m_PCs_test.copy()\n",
    "    test = test.merge(target_terciles.loc[:,region_name], left_index=True, right_index=True)\n",
    "\n",
    "    X_train = train.iloc[:,:-1].values\n",
    "    y_train = train.iloc[:,-1].values\n",
    "\n",
    "    X_test = test.iloc[:,:-1].values\n",
    "    y_test = test.iloc[:,-1].values\n",
    "    \n",
    "    X = np.concatenate((X_train, X_test), axis=0)\n",
    "    y = np.concatenate((y_train, y_test), axis=0)\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)\n",
    "    \n",
    "    skf.get_n_splits(X, y)\n",
    "    \n",
    "    acc_score = []\n",
    "\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        knn = KNN(n_neighbors=1, metric='minkowski', weights='distance', p=3)\n",
    "\n",
    "        knn.fit(X_train, y_train)\n",
    "\n",
    "        acc_score.append(knn.score(X_test, y_test))\n",
    "    \n",
    "    acc_score_dict[region_name] = np.array(acc_score) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_score_df = pd.DataFrame(acc_score_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_score_df.describe().loc[['min','mean','max']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_name = 'NNI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_skpca_t2m_PCs_train.copy()\n",
    "train = train.merge(target_terciles.loc[:,region_name], left_index=True, right_index=True)\n",
    "\n",
    "test = df_skpca_t2m_PCs_test.copy()\n",
    "test = test.merge(target_terciles.loc[:,region_name], left_index=True, right_index=True)\n",
    "\n",
    "X_train = train.iloc[:,:-1].values\n",
    "y_train = train.iloc[:,-1].values\n",
    "\n",
    "X_test = test.iloc[:,:-1].values\n",
    "y_test = test.iloc[:,-1].values\n",
    "\n",
    "X = np.concatenate((X_train, X_test), axis=0)\n",
    "y = np.concatenate((y_train, y_test), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogluon as ag\n",
    "from autogluon import TabularPrediction as task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:,'y'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = df.iloc[:270,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = df.iloc[270:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape[0] + test_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = task.Dataset(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
